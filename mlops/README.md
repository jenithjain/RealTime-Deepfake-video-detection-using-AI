# ðŸ¤– MLOps Pipeline for Deepfake Detection

Complete MLOps implementation for training, deploying, and monitoring deepfake detection models.

---

## ðŸ“‹ Overview

This MLOps pipeline handles:
- âœ… **Data Management**: Videos and images
- âœ… **Model Training**: Automated training pipeline
- âœ… **Model Versioning**: Track and compare models
- âœ… **Deployment**: Automated deployment to production
- âœ… **Monitoring**: Real-time performance tracking
- âœ… **Retraining**: Automated retraining on new data

---

## ðŸš€ Quick Start

### 1. Prepare Training Data

```bash
# Place videos in mlops/data/videos/
# Or place images in mlops/data/images/

# Extract frames from videos (if needed)
python mlops/training/extract_frames.py
```

### 2. Train Model

```bash
# Train new model
python mlops/training/train_model.py --config mlops/training/config.yaml

# This will:
# - Load data from mlops/data/
# - Train EfficientNet-B0 model
# - Save model to mlops/registry/models/
# - Log metrics
```

### 3. Evaluate Model

```bash
# Evaluate model on test set
python mlops/evaluation/evaluate.py --model v1.0.0

# Compare with production model
python mlops/evaluation/evaluate.py --compare v1.0.0 v1.1.0
```

### 4. Deploy Model

```bash
# Deploy to staging
python mlops/deployment/deploy.py --version v1.0.0 --env staging

# Deploy to production
python mlops/deployment/deploy.py --version v1.0.0 --env production
```

### 5. Monitor Production

```bash
# Start monitoring
python mlops/monitoring/monitor.py

# View metrics
python mlops/monitoring/monitor.py --report
```

---

## ðŸ“Š Pipeline Workflow

```
1. DATA COLLECTION
   â”œâ”€â”€ Upload videos to mlops/data/videos/
   â”œâ”€â”€ Or upload images to mlops/data/images/
   â””â”€â”€ Extract frames (if videos)
   
2. TRAINING
   â”œâ”€â”€ Load data
   â”œâ”€â”€ Train EfficientNet model
   â”œâ”€â”€ Validate on test set
   â””â”€â”€ Save model with version
   
3. EVALUATION
   â”œâ”€â”€ Calculate metrics (accuracy, F1, AUC)
   â”œâ”€â”€ Compare with current production model
   â””â”€â”€ Generate evaluation report
   
4. DEPLOYMENT
   â”œâ”€â”€ If metrics improved â†’ Deploy to staging
   â”œâ”€â”€ Run integration tests
   â””â”€â”€ If tests pass â†’ Deploy to production
   
5. MONITORING
   â”œâ”€â”€ Track predictions in real-time
   â”œâ”€â”€ Monitor accuracy, latency
   â”œâ”€â”€ Detect data drift
   â””â”€â”€ Alert if performance drops
   
6. RETRAINING
   â”œâ”€â”€ Collect misclassified examples
   â”œâ”€â”€ Add to training dataset
   â””â”€â”€ Trigger automated retraining
```

---

## ðŸŽ¯ Features

### Model Versioning
- Track all model versions
- Compare model performance
- Rollback to previous versions
- Store model metadata

### Automated Training
- Train on videos or images
- Hyperparameter tuning
- Early stopping
- Checkpoint saving

### Real-time Monitoring
- Track predictions
- Monitor accuracy
- Detect data drift
- Performance alerts

### A/B Testing
- Deploy multiple models
- Split traffic
- Compare performance
- Promote best model

---

## ðŸ“ Folder Structure

```
mlops/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ videos/              # Training videos (real & fake)
â”‚   â”œâ”€â”€ images/              # Extracted frames or images
â”‚   â”œâ”€â”€ train/               # Training data
â”‚   â”œâ”€â”€ val/                 # Validation data
â”‚   â”œâ”€â”€ test/                # Test data
â”‚   â””â”€â”€ metadata.json        # Dataset metadata
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train_model.py       # Main training script
â”‚   â”œâ”€â”€ extract_frames.py    # Extract frames from videos
â”‚   â”œâ”€â”€ config.yaml          # Training configuration
â”‚   â”œâ”€â”€ utils.py             # Training utilities
â”‚   â””â”€â”€ augmentation.py      # Data augmentation
â”‚
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate.py          # Model evaluation
â”‚   â”œâ”€â”€ metrics.py           # Metrics calculation
â”‚   â””â”€â”€ reports/             # Evaluation reports
â”‚
â”œâ”€â”€ registry/
â”‚   â”œâ”€â”€ model_registry.py    # Model versioning system
â”‚   â”œâ”€â”€ models/              # Stored model versions
â”‚   â”‚   â”œâ”€â”€ v1.0.0/
â”‚   â”‚   â”œâ”€â”€ v1.1.0/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ metadata.json        # Model registry metadata
â”‚
â”œâ”€â”€ monitoring/
â”‚   â”œâ”€â”€ monitor.py           # Production monitoring
â”‚   â”œâ”€â”€ drift_detector.py    # Data drift detection
â”‚   â”œâ”€â”€ logs/                # Monitoring logs
â”‚   â””â”€â”€ alerts/              # Alert configurations
â”‚
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ deploy.py            # Deployment script
â”‚   â”œâ”€â”€ rollback.py          # Rollback script
â”‚   â””â”€â”€ config/              # Deployment configs
â”‚
â””â”€â”€ README.md                # This file
```

---

## ðŸ”§ Configuration

### Training Config (training/config.yaml)

```yaml
model:
  architecture: efficientnet-b0
  pretrained: true
  num_classes: 2

training:
  batch_size: 32
  epochs: 50
  learning_rate: 0.001
  optimizer: adam
  early_stopping: true
  patience: 5

data:
  train_split: 0.7
  val_split: 0.15
  test_split: 0.15
  augmentation: true
  image_size: 224

mlops:
  experiment_name: deepfake-detection
  model_version: v1.0.0
  track_metrics: true
```

---

## ðŸ“Š Metrics Tracked

### Training Metrics
- Loss (train & validation)
- Accuracy
- Precision
- Recall
- F1 Score
- AUC-ROC

### Production Metrics
- Prediction latency
- Throughput (predictions/sec)
- Error rate
- Confidence distribution
- Data drift score

---

## ðŸŽ¯ Use Cases

### 1. Train New Model
```bash
python mlops/training/train_model.py \
    --data mlops/data/train \
    --version v1.0.0 \
    --epochs 50
```

### 2. Evaluate Model
```bash
python mlops/evaluation/evaluate.py \
    --model v1.0.0 \
    --test-data mlops/data/test
```

### 3. Compare Models
```bash
python mlops/evaluation/evaluate.py \
    --compare v1.0.0 v1.1.0
```

### 4. Deploy to Production
```bash
python mlops/deployment/deploy.py \
    --version v1.1.0 \
    --env production
```

### 5. Monitor Production
```bash
python mlops/monitoring/monitor.py \
    --dashboard
```

### 6. Rollback
```bash
python mlops/deployment/rollback.py \
    --to-version v1.0.0
```

---

## ðŸ”„ Automated Retraining

The pipeline automatically retrains when:
- Performance drops below threshold
- Data drift detected
- New training data available
- Scheduled (weekly/monthly)

---

## ðŸ“ˆ Dashboard

Access monitoring dashboard:
```bash
python mlops/monitoring/dashboard.py
# Open http://localhost:8050
```

Shows:
- Real-time predictions
- Model performance
- Data drift
- Alerts

---

## ðŸš¨ Alerts

Configured alerts for:
- Accuracy drop > 5%
- Latency > 500ms
- Error rate > 1%
- Data drift detected

---

## ðŸŽ‰ Next Steps

1. âœ… Add training data to `mlops/data/`
2. âœ… Configure training in `mlops/training/config.yaml`
3. âœ… Train first model: `python mlops/training/train_model.py`
4. âœ… Evaluate model: `python mlops/evaluation/evaluate.py`
5. âœ… Deploy to production: `python mlops/deployment/deploy.py`
6. âœ… Monitor: `python mlops/monitoring/monitor.py`

**Your MLOps pipeline is ready!** ðŸš€
